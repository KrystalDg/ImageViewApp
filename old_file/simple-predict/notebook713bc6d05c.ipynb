{"cells":[{"cell_type":"markdown","metadata":{"id":"ImHu0JlkvPQe"},"source":["# Vietnamese Handwritten Recognition with CRNN model"]},{"cell_type":"markdown","metadata":{"id":"SQq9T32UvKvK"},"source":["## Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:01:43.734346Z","iopub.status.busy":"2023-10-10T12:01:43.733962Z","iopub.status.idle":"2023-10-10T12:01:48.724910Z","shell.execute_reply":"2023-10-10T12:01:48.723907Z","shell.execute_reply.started":"2023-10-10T12:01:43.734320Z"},"id":"gobvId0igACm","trusted":true},"outputs":[],"source":["import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","    # RIP Training time\n","    raise SystemError('GPU device not found')\n","    print('Found GPU at: {}'.format(device_name))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:01:48.728105Z","iopub.status.busy":"2023-10-10T12:01:48.727072Z","iopub.status.idle":"2023-10-10T12:01:48.735587Z","shell.execute_reply":"2023-10-10T12:01:48.735027Z","shell.execute_reply.started":"2023-10-10T12:01:48.728068Z"},"id":"QQ8smevAfnJK","outputId":"615ead43-addb-4106-8b95-55abedd50a24","trusted":true},"outputs":[],"source":["import pathlib\n","# finding where we are so we dont need to type absolute path everytime\n","current_directory_path = pathlib.Path(\"/kaggle/input/vn-handwritten-images\").absolute()\n","current_directory_path\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:01:48.737479Z","iopub.status.busy":"2023-10-10T12:01:48.736837Z","iopub.status.idle":"2023-10-10T12:01:48.748076Z","shell.execute_reply":"2023-10-10T12:01:48.747163Z","shell.execute_reply.started":"2023-10-10T12:01:48.737447Z"},"id":"-V5lU-mAfwQf","trusted":true},"outputs":[],"source":["import os\n","# path to our directory images\n","\n","# our train and test folder (not used for now but used later when we want to save images after processing)\n","DATA_PATH = os.path.join(str(current_directory_path), \"data\")\n","TEST_FOLDER = os.path.join(DATA_PATH, \"test\")\n","TRAIN_FOLDER = os.path.join(DATA_PATH, \"train\")\n","\n","# our raw folders\n","RAW_FOLDER = os.path.join(str(current_directory_path),\"vn_handwritten_images\")\n","\n","# label json files\n","TRAIN_JSON = os.path.join(RAW_FOLDER, \"labels.json\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:01:48.751500Z","iopub.status.busy":"2023-10-10T12:01:48.750837Z","iopub.status.idle":"2023-10-10T12:01:48.760908Z","shell.execute_reply":"2023-10-10T12:01:48.759926Z","shell.execute_reply.started":"2023-10-10T12:01:48.751469Z"},"id":"5M4vq80xiEch","outputId":"33b7ab52-3505-4573-caf4-7ccb6f023dc2","trusted":true},"outputs":[],"source":["RAW_FOLDER\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:01:48.763008Z","iopub.status.busy":"2023-10-10T12:01:48.762114Z","iopub.status.idle":"2023-10-10T12:01:48.773733Z","shell.execute_reply":"2023-10-10T12:01:48.772830Z","shell.execute_reply.started":"2023-10-10T12:01:48.762955Z"},"id":"sEfX9MZJiFlC","outputId":"5ebe294f-a36c-42fc-f19f-60617f969e29","trusted":true},"outputs":[],"source":["TRAIN_JSON\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:01:48.775597Z","iopub.status.busy":"2023-10-10T12:01:48.774906Z","iopub.status.idle":"2023-10-10T12:01:48.788750Z","shell.execute_reply":"2023-10-10T12:01:48.787769Z","shell.execute_reply.started":"2023-10-10T12:01:48.775568Z"},"id":"jQi4Av8ahHmS","trusted":true},"outputs":[],"source":["import json\n","# open these label jsons files\n","with open(TRAIN_JSON, 'r', encoding='utf8') as f:\n","    train_labels = json.load(f)\n"]},{"cell_type":"markdown","metadata":{"id":"Nf7ScbfQ1y4s"},"source":["Here are the labels for each image!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:01:48.791030Z","iopub.status.busy":"2023-10-10T12:01:48.790291Z","iopub.status.idle":"2023-10-10T12:01:48.794899Z","shell.execute_reply":"2023-10-10T12:01:48.794017Z","shell.execute_reply.started":"2023-10-10T12:01:48.790879Z"},"id":"0neIfFZVhTaA","outputId":"9d710ae7-dd5c-4caa-e1a9-a4751d8a77f1","trusted":true},"outputs":[],"source":["# train_labels\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:01:48.797161Z","iopub.status.busy":"2023-10-10T12:01:48.796402Z","iopub.status.idle":"2023-10-10T12:01:48.813441Z","shell.execute_reply":"2023-10-10T12:01:48.812750Z","shell.execute_reply.started":"2023-10-10T12:01:48.797130Z"},"id":"VrInI1tUiL8X","outputId":"8651b76f-8c42-4238-f731-bfdaf046297e","trusted":true},"outputs":[],"source":["#find all characters in labels (so we don't blindly add chacters not even existed in our dataset)\n","char_list= set()\n","for label in train_labels.values():\n","    char_list.update(set(label))\n","char_list=sorted(char_list)\n","len(char_list)\n"]},{"cell_type":"markdown","metadata":{"id":"rJOlsM7qiaR6"},"source":["So there are up to 140 unique characters in this Vietnamese dataset for addresses! Very interesting!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:01:48.815458Z","iopub.status.busy":"2023-10-10T12:01:48.814696Z","iopub.status.idle":"2023-10-10T12:01:48.825484Z","shell.execute_reply":"2023-10-10T12:01:48.824548Z","shell.execute_reply.started":"2023-10-10T12:01:48.815428Z"},"id":"cZ2OGDAKiSc0","outputId":"c890605c-1ae3-4a50-e946-d3ce569d4628","trusted":true},"outputs":[],"source":["# show all possible labels characters\n","\"\".join(char_list)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:01:48.829911Z","iopub.status.busy":"2023-10-10T12:01:48.829636Z","iopub.status.idle":"2023-10-10T12:01:48.836444Z","shell.execute_reply":"2023-10-10T12:01:48.835499Z","shell.execute_reply.started":"2023-10-10T12:01:48.829890Z"},"id":"r0X5dBP-i1C_","trusted":true},"outputs":[],"source":["# convert the words to array of indexs based on the char_list\n","def encode_to_labels(txt):\n","    # encoding each output word into digits of indexes\n","    dig_lst = []\n","    for index, char in enumerate(txt):\n","        try:\n","            dig_lst.append(char_list.index(char))\n","        except:\n","            print(\"No found in char_list :\", char)\n","\n","    return dig_lst\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:01:48.838817Z","iopub.status.busy":"2023-10-10T12:01:48.837605Z","iopub.status.idle":"2023-10-10T12:01:48.851023Z","shell.execute_reply":"2023-10-10T12:01:48.850031Z","shell.execute_reply.started":"2023-10-10T12:01:48.838784Z"},"id":"88Q6407ni2TV","outputId":"2dd1f672-3cbd-4624-9824-49b8178a04a5","trusted":true},"outputs":[],"source":["encode_to_labels(\"Life is hard\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:01:48.853145Z","iopub.status.busy":"2023-10-10T12:01:48.852356Z","iopub.status.idle":"2023-10-10T12:01:49.814550Z","shell.execute_reply":"2023-10-10T12:01:49.813548Z","shell.execute_reply.started":"2023-10-10T12:01:48.853116Z"},"id":"60z8FX6NjIFG","trusted":true},"outputs":[],"source":["# find all image paths for train folder 1 and 2 and test folder\n","train_image_path = []\n","\n","for item in pathlib.Path(RAW_FOLDER).glob('**/*'):\n","    if item.is_file() and item.suffix not in [\".json\"]:\n","        train_image_path.append(str(item))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:01:49.816183Z","iopub.status.busy":"2023-10-10T12:01:49.815814Z","iopub.status.idle":"2023-10-10T12:01:49.822592Z","shell.execute_reply":"2023-10-10T12:01:49.821493Z","shell.execute_reply.started":"2023-10-10T12:01:49.816151Z"},"id":"nes9YOwBjOQO","outputId":"9e06e633-15ad-448e-ffa4-10da6999e02a","trusted":true},"outputs":[],"source":["train_image_path[:10]\n"]},{"cell_type":"markdown","metadata":{"id":"iIYW2Xwxi4ib"},"source":["You can see that there is no f in Vietnamese language, we are so nice as people ! What are other characters not existing in Vietnamese?"]},{"cell_type":"markdown","metadata":{"id":"adf1rEjMvCoJ"},"source":["## Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"AnQVc2JpjZmO"},"source":["#### Here what we gonna do now:\n","- Find all widths and heights of images\n","- Use openCV to read image \n","- Preprocess images (like converting images to greyscale)\n","- Resize images so all images will have the same size\n","- Split your dataset into trainset and testset\n","- Build CRNN model with CTC loss\n","- Prediction\n","- Calculate metrics for SER, WER and CER"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:01:49.825471Z","iopub.status.busy":"2023-10-10T12:01:49.824234Z","iopub.status.idle":"2023-10-10T12:01:49.856982Z","shell.execute_reply":"2023-10-10T12:01:49.856208Z","shell.execute_reply.started":"2023-10-10T12:01:49.825440Z"},"id":"Rom3nHrjNfkT","trusted":true},"outputs":[],"source":["# YOUR CODE: Continue to preprocess dataset and create CRNN model with CTC loss!\n","\n","#Matching full absoluate paths and labels instead of filenames and labels\n","dict_filepath_label={}\n","raw_data_path = pathlib.Path(os.path.join(RAW_FOLDER))\n","for item in raw_data_path.glob('**/*.*'):\n","    file_name=str(os.path.basename(item))\n","    if file_name != \"labels.json\":\n","      label = train_labels[file_name]\n","      dict_filepath_label[str(item)]=label\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:01:49.858379Z","iopub.status.busy":"2023-10-10T12:01:49.858070Z","iopub.status.idle":"2023-10-10T12:01:49.862543Z","shell.execute_reply":"2023-10-10T12:01:49.861603Z","shell.execute_reply.started":"2023-10-10T12:01:49.858350Z"},"id":"a4Tueuj1OmCs","outputId":"79d8879b-bced-43af-c571-12afcfe88da0","trusted":true},"outputs":[],"source":["# dict_filepath_label\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:01:49.864618Z","iopub.status.busy":"2023-10-10T12:01:49.863985Z","iopub.status.idle":"2023-10-10T12:01:49.874345Z","shell.execute_reply":"2023-10-10T12:01:49.873377Z","shell.execute_reply.started":"2023-10-10T12:01:49.864588Z"},"id":"_ORMvAaHiTQJ","outputId":"1740636a-ea1d-453a-e19b-b5b4e0c02e56","trusted":true},"outputs":[],"source":["# find the maximum label length\n","label_lens= []\n","for label in dict_filepath_label.values():\n","    label_lens.append(len(label))\n","max_label_len = max(label_lens)\n","\n","max_label_len\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:01:49.876097Z","iopub.status.busy":"2023-10-10T12:01:49.875549Z","iopub.status.idle":"2023-10-10T12:01:49.884718Z","shell.execute_reply":"2023-10-10T12:01:49.883796Z","shell.execute_reply.started":"2023-10-10T12:01:49.876069Z"},"id":"h5SXaY97iulm","trusted":true},"outputs":[],"source":["all_image_paths = list(dict_filepath_label.keys())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:01:49.886559Z","iopub.status.busy":"2023-10-10T12:01:49.885998Z","iopub.status.idle":"2023-10-10T12:01:49.897105Z","shell.execute_reply":"2023-10-10T12:01:49.896034Z","shell.execute_reply.started":"2023-10-10T12:01:49.886531Z"},"id":"M9H-UwGpM_0E","outputId":"2e51c2f3-df8a-4366-969e-d225a395c546","trusted":true},"outputs":[],"source":["all_image_paths[:10]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:01:49.899200Z","iopub.status.busy":"2023-10-10T12:01:49.898613Z","iopub.status.idle":"2023-10-10T12:02:02.483485Z","shell.execute_reply":"2023-10-10T12:02:02.482508Z","shell.execute_reply.started":"2023-10-10T12:01:49.899171Z"},"id":"cjoWWkvdM7Zt","trusted":true},"outputs":[],"source":["import cv2\n","# find all widths and heights of images (this is useful if our dataset images got different sizes)\n","widths = []\n","heights = []\n","for image_path in all_image_paths:\n","    img = cv2.imread(image_path)\n","    (height, width, _) = img.shape\n","    heights.append(height)\n","    widths.append(width)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:02:02.485437Z","iopub.status.busy":"2023-10-10T12:02:02.485096Z","iopub.status.idle":"2023-10-10T12:02:02.490541Z","shell.execute_reply":"2023-10-10T12:02:02.489654Z","shell.execute_reply.started":"2023-10-10T12:02:02.485409Z"},"id":"sQHlsyzWNO4l","trusted":true},"outputs":[],"source":["min_height = min(heights)\n","max_height = max(heights)\n","min_width = min(widths)\n","max_width = max(widths)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:02:02.492593Z","iopub.status.busy":"2023-10-10T12:02:02.491689Z","iopub.status.idle":"2023-10-10T12:02:02.506462Z","shell.execute_reply":"2023-10-10T12:02:02.505451Z","shell.execute_reply.started":"2023-10-10T12:02:02.492564Z"},"id":"V9VQ3h02PIad","outputId":"919ad4fd-650f-49af-a8ad-0ce631780a88","trusted":true},"outputs":[],"source":["# this information is useful for making decision for padding and resizing\n","(min_height, max_height, min_width, max_width)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:02:02.508428Z","iopub.status.busy":"2023-10-10T12:02:02.507841Z","iopub.status.idle":"2023-10-10T12:02:02.782713Z","shell.execute_reply":"2023-10-10T12:02:02.781741Z","shell.execute_reply.started":"2023-10-10T12:02:02.508398Z"},"id":"1NyJRwUvPKV6","trusted":true},"outputs":[],"source":["# being a good Data Scientist, we need to have train set and test set\n","from sklearn.model_selection import train_test_split\n","test_size = 0.2\n","train_image_paths, val_image_paths = train_test_split(all_image_paths, test_size=test_size, random_state=42)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:02:02.784515Z","iopub.status.busy":"2023-10-10T12:02:02.784199Z","iopub.status.idle":"2023-10-10T12:02:02.788796Z","shell.execute_reply":"2023-10-10T12:02:02.787833Z","shell.execute_reply.started":"2023-10-10T12:02:02.784486Z"},"id":"Vwy_y2OgQQmx","trusted":true},"outputs":[],"source":["# Constants (we choose this number as we later discover that the output of our CNN is (1,9,512) from out input)\n","# TIME_STEPS = 240\n","TIME_STEPS = 80\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:02:02.790654Z","iopub.status.busy":"2023-10-10T12:02:02.790108Z","iopub.status.idle":"2023-10-10T12:02:02.803184Z","shell.execute_reply":"2023-10-10T12:02:02.802312Z","shell.execute_reply.started":"2023-10-10T12:02:02.790624Z"},"trusted":true},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import csv\n","import datetime\n","import random\n","import pathlib\n","import json\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import applications\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.activations import relu, sigmoid, softmax\n","from tensorflow.keras.callbacks import CSVLogger, TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n","from tensorflow.keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional, Add, Activation\n","\n","from scipy.ndimage.interpolation import map_coordinates\n","from scipy.ndimage.filters import gaussian_filter\n","\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import GridSearchCV, train_test_split, RepeatedStratifiedKFold, cross_val_score, KFold,StratifiedKFold\n","# import pyprind\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","random.seed(2023)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:02:02.805488Z","iopub.status.busy":"2023-10-10T12:02:02.804800Z","iopub.status.idle":"2023-10-10T12:02:02.821941Z","shell.execute_reply":"2023-10-10T12:02:02.821082Z","shell.execute_reply.started":"2023-10-10T12:02:02.805455Z"},"trusted":true},"outputs":[],"source":["def preprocess1(img, imgSize):\n","    ''' resize, transpose and standardization grayscale images '''\n","    # create target image and copy sample image into it\n","\n","    widthTarget, heightTarget = imgSize\n","    height, width= img.shape\n","    factor_x = width / widthTarget\n","    factor_y = height / heightTarget\n","    factor = max(factor_x, factor_y)\n","\n","    # scale according to factor\n","    newSize = (min(widthTarget, int(width / factor)), min(heightTarget, int(height / factor)))\n","    #print ('newSize ={}, old size = {}'.format(newSize, img.shape ))\n","    img = cv2.resize(img, newSize)\n","    target = np.ones(shape=(heightTarget, widthTarget), dtype='uint8') * 255 #tao ma tran 255 (128,32)\n","    target[0:newSize[1], 0:newSize[0]] = img #Padding trên hoặc dưới\n","\n","    #transpose\n","    img = cv2.transpose(target)\n","    img = cv2.transpose(img)\n","\n","    # # standardization\n","    # mean, stddev = cv2.meanStdDev(img)\n","    # mean = mean[0][0]\n","    # stddev = stddev[0][0] # standard deviation\n","    # #print ('mean ={}, stddev = {}'.format(mean, stddev))\n","    # img = img - mean\n","    # img = img // (stddev) if stddev > 0 else img\n","    # #print ('set', set(img.flatten()))\n","    # #img out co shape (128,32)\n","    return img\n","\n","def elastic_transform(image, alpha, sigma, random_state=None):\n","    if random_state is None:\n","        random_state = np.random.RandomState(None)\n","\n","    h, w = image.shape[:2]\n","    x, y = np.meshgrid(np.arange(w), np.arange(h))\n","    dx = gaussian_filter((random_state.rand(h,w) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n","    dy = gaussian_filter((random_state.rand(h,w) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n","    indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1))\n","\n","    distored_image = map_coordinates(image, indices, order=1, mode='reflect')\n","\n","    return distored_image.reshape(image.shape)\n","\n","def augmentation(image):\n","    ''' The dataset is so clean, so it's not fit with out real input images with blobs, noise, ugly characters '''\n","    height, width = image.shape\n","\n","    # dilate/erode image (optional)\n","    method = random.randint(0,2)\n","    kernelsize = [(3,1), (1,3), (3,3), (5,5), (6,2), (2,6), (1,5)]\n","    #print ('kernel size', random.sample(kernelsize,1))\n","    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, random.sample(kernelsize,1)[0])\n","    if method == 1:\n","        image = cv2.erode(image, kernel)\n","    elif method == 2:\n","        image = cv2.dilate(image, kernel)\n","\n","\n","    # add blob noise\n","    num_blob= int(random.gauss(10,10))\n","    for i in range (num_blob):\n","        x = random.randint(0,width)\n","        y = int (random.gauss(0,height))\n","        radius = random.randint(0,5)\n","\n","        image = cv2.circle(image, (x,y), radius = radius, color = ((255,255,255) ), thickness = -2)\n","\n","    # add line noise\n","    num_line = random.randint(0,2)\n","    for i in range(num_line):\n","        p1 = (random.randint(0,15), random.randint(height-40,height)) # (x,y)\n","        p2 = (random.randint(width-15,width), random.randint(height-40,height)) # (x,y)\n","        cv2.line(image, p1, p2, (255,255,255), thickness = random.randint(1,3))\n","\n","    # #Random vertical cut out\n","    num_cut = random.randint(0,5)\n","    for i in range(num_cut):\n","        p1 = (random.randint(0,width), 0) # (x,y)\n","        p2 = (p1[0]+10, p1[1]+300) # (x,y)\n","        cv2.rectangle(image, p1, p2, (255,255,255), 1)\n","\n","    #Random horizontal cut out\n","    num_line = random.randint(0,3)\n","    for i in range(num_line):\n","        p1 = (random.randint(0,15), random.randint(height-40,height)) # (x,y)\n","        p2 = (random.randint(width-15,width), random.randint(height-40,height)) # (x,y)\n","        cv2.line(image, p1, p2, (255, 255, 255), thickness = random.randint(1,3))\n","\n","    #Elastic transform\n","    chance  = random.randint(0,100)\n","    if chance < 10: # Chỉ 10% data được elastic transform\n","        image  =  elastic_transform(image, image.shape[1] * 2, image.shape[1] * 0.08)\n","\n","    return image\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:02:02.839413Z","iopub.status.busy":"2023-10-10T12:02:02.838870Z","iopub.status.idle":"2023-10-10T12:02:26.962564Z","shell.execute_reply":"2023-10-10T12:02:26.961469Z","shell.execute_reply.started":"2023-10-10T12:02:02.839385Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","# lists for training dataset\n","training_img = []\n","training_txt = []\n","train_input_length = []\n","train_label_length = []\n","orig_txt = []\n","resize_max_width=0\n","i=0\n","for train_img_path in train_image_paths:\n","    # print(f_name)\n","    # read input image and convert into gray scale image\n","    img = cv2.cvtColor(cv2.imread(train_img_path), cv2.COLOR_BGR2GRAY)\n","\n","    height, width = img.shape\n","\n","    # in this dataset, we don't need to do any resize at all here.\n","    img = cv2.resize(img,(int(118/height*width),118))\n","\n","    height, width = img.shape\n","\n","    if img.shape[1] > resize_max_width:\n","        resize_max_width = img.shape[1]\n","\n","    img = np.pad(img, ((0,0),(0, 2167-width)), 'median')\n","\n","    # YOUR PART: Blur it\n","    img = cv2.GaussianBlur(img, (5,5), 0)\n","\n","    # YOUR PART: Threshold the image using adapative threshold\n","    img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 4)\n","\n","    # add channel dimension\n","    img = np.expand_dims(img , axis = 2)\n","\n","    # Normalize each image\n","#     img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n","    img = img/255.\n","\n","    label = dict_filepath_label[train_img_path]\n","\n","    # split data into validation and training dataset as 10% and 90% respectively\n","    orig_txt.append(label)\n","    train_label_length.append(len(label))\n","\n","    # our time steps for valid input\n","    train_input_length.append(TIME_STEPS)\n","    training_img.append(img)\n","\n","    # convert words to digits based on charlist\n","    training_txt.append(encode_to_labels(label))\n","    i+=1\n","    if (i%500 == 0):\n","        print (\"has processed trained {} files\".format(i))\n","print(resize_max_width)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:02:26.968917Z","iopub.status.busy":"2023-10-10T12:02:26.968299Z","iopub.status.idle":"2023-10-10T12:02:28.037794Z","shell.execute_reply":"2023-10-10T12:02:28.036837Z","shell.execute_reply.started":"2023-10-10T12:02:26.968884Z"},"id":"7Z21v4qgPoCD","outputId":"1363feb6-bfcb-42ec-bcd9-74aef608665f","trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","for i in range(5):\n","    plt.figure(figsize=(15,2))\n","    plt.imshow(training_img[i], cmap=\"gray\")\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:02:28.047634Z","iopub.status.busy":"2023-10-10T12:02:28.046945Z","iopub.status.idle":"2023-10-10T12:02:33.953062Z","shell.execute_reply":"2023-10-10T12:02:33.951924Z","shell.execute_reply.started":"2023-10-10T12:02:28.047546Z"},"trusted":true},"outputs":[],"source":["#lists for validation dataset\n","valid_img = []\n","valid_txt = []\n","valid_input_length = []\n","valid_label_length = []\n","valid_orig_txt = []\n","\n","i=0\n","\n","for val_img_path in val_image_paths:\n","    # print(f_name)\n","    # read input image and convert into gray scale image\n","    img = cv2.cvtColor(cv2.imread(val_img_path), cv2.COLOR_BGR2GRAY)\n","\n","    # in this dataset, we don't need to do any resize at all here.\n","    img = cv2.resize(img,(int(118/height*width),118))\n","\n","    if img.shape[1] > resize_max_width:\n","        resize_max_width = img.shape[1]\n","\n","    img = np.pad(img, ((0,0),(0, 2167-width)), 'median')\n","\n","    # YOUR PART: Blur it\n","    img = cv2.GaussianBlur(img, (5,5), 0)\n","\n","    # YOUR PART: Threshold the image using adapative threshold\n","    img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 4)\n","\n","    # add channel dimension\n","    img = np.expand_dims(img , axis = 2)\n","\n","    # Normalize each image\n","#     img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n","    img = img/255.\n","\n","    label = dict_filepath_label[val_img_path]\n","\n","    valid_orig_txt.append(label)\n","    valid_label_length.append(len(label))\n","\n","    # our time steps for valid input\n","    valid_input_length.append(TIME_STEPS)\n","    valid_img.append(img)\n","\n","    # convert words to digits based on charlist\n","    valid_txt.append(encode_to_labels(label))\n","    i+=1\n","    if (i%500 == 0):\n","        print (\"has processed test {} files\".format(i))\n","print(resize_max_width)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:02:33.954864Z","iopub.status.busy":"2023-10-10T12:02:33.954289Z","iopub.status.idle":"2023-10-10T12:02:35.002215Z","shell.execute_reply":"2023-10-10T12:02:35.001289Z","shell.execute_reply.started":"2023-10-10T12:02:33.954832Z"},"id":"zSmWTqVdQjrS","outputId":"fdf47264-7c77-4f18-f3d3-03d7b8cd90ae","trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","for i in range(5):\n","    plt.figure(figsize=(15,2))\n","#     plt.imshow(valid_img[i][:,:,0], cmap=\"gray\")\n","    plt.imshow(valid_img[i], cmap=\"gray\")\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:02:35.004241Z","iopub.status.busy":"2023-10-10T12:02:35.003647Z","iopub.status.idle":"2023-10-10T12:02:35.008659Z","shell.execute_reply":"2023-10-10T12:02:35.007717Z","shell.execute_reply.started":"2023-10-10T12:02:35.004207Z"},"id":"uDuJdInkQo6p","trusted":true},"outputs":[],"source":["# this is the most controversial part when our max_label_len should be set different or slightly smaller than TIME_STEPS\n","# but let stick to the conventional/normal way: we should to be the same with our TIME_STEPS\n","max_label_len = TIME_STEPS\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:02:35.011050Z","iopub.status.busy":"2023-10-10T12:02:35.009995Z","iopub.status.idle":"2023-10-10T12:02:35.030118Z","shell.execute_reply":"2023-10-10T12:02:35.029190Z","shell.execute_reply.started":"2023-10-10T12:02:35.011019Z"},"id":"5Yh9wwIlQvMK","trusted":true},"outputs":[],"source":["# import padding library\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# pad each output label to maximum text length, remember we did that so that we keep training with rnn consistent?\n","train_padded_txt = pad_sequences(training_txt, maxlen=max_label_len, padding='post', value = 0)\n","valid_padded_txt = pad_sequences(valid_txt, maxlen=max_label_len, padding='post', value = 0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:02:35.031634Z","iopub.status.busy":"2023-10-10T12:02:35.031335Z","iopub.status.idle":"2023-10-10T12:02:35.038422Z","shell.execute_reply":"2023-10-10T12:02:35.037347Z","shell.execute_reply.started":"2023-10-10T12:02:35.031605Z"},"id":"NkvpSOBOQwA6","outputId":"bda432d8-f05f-4cc1-fc52-ba81be96d34d","trusted":true},"outputs":[],"source":["train_padded_txt[0]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:02:35.040363Z","iopub.status.busy":"2023-10-10T12:02:35.039774Z","iopub.status.idle":"2023-10-10T12:02:36.083196Z","shell.execute_reply":"2023-10-10T12:02:36.082186Z","shell.execute_reply.started":"2023-10-10T12:02:35.040334Z"},"trusted":true},"outputs":[],"source":["# notice I convert everything to numpy array here since model can fit either tensors and numpy array\n","# the reason why i stick with numpy array because it is simple to code and also because it is baseline solution\n","# I want to use openCV (can't run with tensors because it is CPU operations)\n","# And if I want to use tensors then i will make sure my image preprocessing functions uses tensor compatible operations\n","\n","# ready our training data\n","training_img = np.array(training_img)\n","train_input_length = np.array(train_input_length)  # all must be equal length to T timesteps\n","train_label_length = np.array(train_label_length)  # different length (only the same in Captcha dataset)\n","\n","# ready our validating data\n","valid_img = np.array(valid_img)\n","valid_input_length = np.array(valid_input_length) # all must be equal length to T timesteps\n","valid_label_length = np.array(valid_label_length) # different length (only the same in Captcha dataset)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:02:36.085260Z","iopub.status.busy":"2023-10-10T12:02:36.084655Z","iopub.status.idle":"2023-10-10T12:02:36.092700Z","shell.execute_reply":"2023-10-10T12:02:36.091653Z","shell.execute_reply.started":"2023-10-10T12:02:36.085228Z"},"trusted":true},"outputs":[],"source":["training_img.shape, valid_img.shape, train_padded_txt.shape, valid_padded_txt.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:02:36.095012Z","iopub.status.busy":"2023-10-10T12:02:36.094239Z","iopub.status.idle":"2023-10-10T12:02:36.101807Z","shell.execute_reply":"2023-10-10T12:02:36.100886Z","shell.execute_reply.started":"2023-10-10T12:02:36.094955Z"},"trusted":true},"outputs":[],"source":["train_data = [training_img, train_padded_txt, train_input_length, train_label_length]\n","valid_data = [valid_img, valid_padded_txt, valid_input_length, valid_label_length]\n"]},{"cell_type":"markdown","metadata":{"id":"UdTJCXxXQ6Ca"},"source":["## Model Building"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:02:36.161410Z","iopub.status.busy":"2023-10-10T12:02:36.160802Z","iopub.status.idle":"2023-10-10T12:02:38.235605Z","shell.execute_reply":"2023-10-10T12:02:38.234842Z","shell.execute_reply.started":"2023-10-10T12:02:36.161381Z"},"trusted":true},"outputs":[],"source":["# import our model, different layers and activation function\n","from tensorflow.keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional, Add, Activation\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.activations import relu, sigmoid, softmax\n","import tensorflow.keras.backend as K\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.callbacks import CSVLogger, TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","\n","\n","\n","\n","# OUR FULL MODEL OF CRNN AND LSTM\n","\n","# input with shape of height=32 and width=128\n","inputs = Input(shape=(118,2167,1))\n","\n","# Block 1\n","x = Conv2D(64, (3,3), padding='same')(inputs)\n","x = MaxPool2D(pool_size=3, strides=3)(x)\n","x = Activation('relu')(x)\n","x_1 = x\n","\n","# Block 2\n","x = Conv2D(128, (3,3), padding='same')(x)\n","x = MaxPool2D(pool_size=3, strides=3)(x)\n","x = Activation('relu')(x)\n","x_2 = x\n","\n","# Block 3\n","x = Conv2D(256, (3,3), padding='same')(x)\n","x = BatchNormalization()(x)\n","x = Activation('relu')(x)\n","x_3 = x\n","\n","# Block4\n","x = Conv2D(256, (3,3), padding='same')(x)\n","x = BatchNormalization()(x)\n","x = Add()([x,x_3])\n","x = Activation('relu')(x)\n","x_4 = x\n","\n","# Block5\n","x = Conv2D(512, (3,3), padding='same')(x)\n","x = BatchNormalization()(x)\n","x = Activation('relu')(x)\n","x_5 = x\n","\n","# Block6\n","x = Conv2D(512, (3,3), padding='same')(x)\n","x = BatchNormalization()(x)\n","x = Add()([x,x_5])\n","x = Activation('relu')(x)\n","\n","# Block7\n","x = Conv2D(1024, (3,3), padding='same')(x)\n","x = BatchNormalization()(x)\n","x = MaxPool2D(pool_size=(3, 1))(x)\n","x = Activation('relu')(x)\n","\n","# pooling layer with kernel size (2,2) to make the height/2 #(1,9,512)\n","x = MaxPool2D(pool_size=(3, 1))(x)\n","\n","# # to remove the first dimension of one: (1, 31, 512) to (31, 512)\n","squeezed = Lambda(lambda x: K.squeeze(x, 1))(x)\n","\n","# # # bidirectional LSTM layers with units=128\n","blstm_1 = Bidirectional(LSTM(512, return_sequences=True, dropout = 0.2))(squeezed)\n","blstm_2 = Bidirectional(LSTM(512, return_sequences=True, dropout = 0.2))(blstm_1)\n","\n","# # this is our softmax character proprobility with timesteps\n","outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_2)\n","\n","# model to be used at test time\n","\n","act_model = Model(inputs, outputs)\n","\n","\n","\n","\n","### ctc definition part\n","\"\"\"\n","The reason why we use ctc_batch_cost instead of ctc_cost because\n","In batch cost, the program managed to mask out padded sequence using label_length,\n","So the cost won't include the padding 0 section.\n","\"\"\"\n","\n","\"\"\"\n","The way CTC is modelled currently in Keras is that you need to implement the loss function as a layer, you did that already (loss_out).\n","Your problem is that the inputs you give that layer are not tensors from Theano/TensorFlow but numpy arrays.\n","To change that one option is to model these values as inputs to your model.\n","That is why we need to manually create Input for labels, input length and label_length.\n","\"\"\"\n","\n","# define the label input shape for ctc\n","labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')\n","\n","\"\"\"\n","where input_length and label_length are constants you created previously\n","the easiest way here is to have a fixed batch size in training\n","the lengths should have the same batch size (see shapes in the link for ctc_cost)\n","\"\"\"\n","\n","# define the length of input and label for ctc\n","input_length = Input(name='input_length', shape=[1], dtype='int64')\n","label_length = Input(name='label_length', shape=[1], dtype='int64')\n","\n","# define a ctc lambda function to take arguments and return ctc_bach_cost\n","def ctc_lambda_func(args):\n","    y_pred, labels, input_length, label_length = args\n","    \"\"\"\n","    labels: tensor (number of samples, max_string_length) containing the truth labels.\n","    y_pred: tensor (number of samples, time_steps, num_character_labels) containing the prediction, or output of the softmax.\n","    input_length: tensor (number of samples, 1) containing the sequence length for each batch item in y_pred.\n","    label_length: tensor (number of samples, 1) containing the sequence length for each batch item in y_true.\n","    \"\"\"\n","    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n","\n","# out loss function (just take the inputs and put it in our ctc_batch_cost)\n","loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, labels, input_length, label_length])\n","\n","#model to be used at training time\n","model = Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)\n","\n","\n","# ready ctc loss function and optimizers\n","model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = 'adam')\n","\n","\n","model.summary()\n"]},{"cell_type":"markdown","metadata":{},"source":["## TRAIN"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:02:38.237801Z","iopub.status.busy":"2023-10-10T12:02:38.237330Z","iopub.status.idle":"2023-10-10T12:59:22.607419Z","shell.execute_reply":"2023-10-10T12:59:22.606389Z","shell.execute_reply.started":"2023-10-10T12:02:38.237765Z"},"trusted":true},"outputs":[],"source":["# our callbacks hell to optimize our learning\n","callbacks = [\n","    TensorBoard(\n","        log_dir='./logs',\n","        histogram_freq=10,\n","        profile_batch=0,\n","        write_graph=True,\n","        write_images=False,\n","        update_freq=\"epoch\"),\n","    ModelCheckpoint(\n","        filepath=os.path.join('checkpoint_weights.hdf5'),\n","        monitor='val_loss',\n","        save_best_only=True,\n","        save_weights_only=True,\n","        verbose=1),\n","    EarlyStopping(\n","        monitor='val_loss',\n","        min_delta=1e-8,\n","        patience=20,\n","        restore_best_weights=True,\n","        verbose=1),\n","    ReduceLROnPlateau(\n","        monitor='val_loss',\n","        min_delta=1e-8,\n","        factor=0.2,\n","        patience=10,\n","        verbose=1)\n","]\n","callbacks_list = callbacks\n","\n","\n","# choose batchsize and epochs\n","batch_size = 32\n","epochs = 200\n","\n","history = model.fit(x=[training_img, train_padded_txt, train_input_length, train_label_length],\n","          y=np.zeros(len(training_img)),\n","          batch_size=batch_size,\n","          epochs = epochs,\n","          validation_data = ([valid_img, valid_padded_txt, valid_input_length, valid_label_length], [np.zeros(len(valid_img))]),\n","          verbose = 1, callbacks = callbacks_list)\n"]},{"cell_type":"markdown","metadata":{"id":"njYW35MAu80d"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T12:59:22.624091Z","iopub.status.busy":"2023-10-10T12:59:22.623233Z","iopub.status.idle":"2023-10-10T13:00:11.156158Z","shell.execute_reply":"2023-10-10T13:00:11.155212Z","shell.execute_reply.started":"2023-10-10T12:59:22.624061Z"},"trusted":true},"outputs":[],"source":["plt.plot(history.history[\"loss\"])\n","plt.plot(history.history[\"val_loss\"])\n","plt.title(\"Training Loss vs Validation Loss\")\n","plt.ylabel(\"Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T13:03:30.592775Z","iopub.status.busy":"2023-10-10T13:03:30.592401Z","iopub.status.idle":"2023-10-10T13:03:30.597532Z","shell.execute_reply":"2023-10-10T13:03:30.596289Z","shell.execute_reply.started":"2023-10-10T13:03:30.592748Z"},"trusted":true},"outputs":[],"source":["# prediction_model = keras.models.Model(model.get_layer(name=\"image\").input, model.get_layer(name=\"output\").output)\n","# prediction_model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-10T13:00:11.745751Z","iopub.status.idle":"2023-10-10T13:00:11.747247Z","shell.execute_reply":"2023-10-10T13:00:11.747026Z","shell.execute_reply.started":"2023-10-10T13:00:11.746999Z"},"id":"RiAB7HezRWVC","trusted":true},"outputs":[],"source":["# load the saved best model weights\n","# act_model.load_weights(os.path.join('checkpoint_weights.hdf5'))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T13:03:45.442421Z","iopub.status.busy":"2023-10-10T13:03:45.442075Z","iopub.status.idle":"2023-10-10T13:03:52.118367Z","shell.execute_reply":"2023-10-10T13:03:52.117360Z","shell.execute_reply.started":"2023-10-10T13:03:45.442395Z"},"id":"Ba9KAXKNX1Am","trusted":true},"outputs":[],"source":["# predict outputs on validation images\n","NO_PREDICTS = 100\n","OFFSET=0\n","# prediction = act_model.predict(valid_img[OFFSET:OFFSET+NO_PREDICTS])\n","# prediction = act_model.predict(valid_img)\n","prediction = act_model.predict(valid_img)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T13:04:06.010946Z","iopub.status.busy":"2023-10-10T13:04:06.009892Z","iopub.status.idle":"2023-10-10T13:04:06.017364Z","shell.execute_reply":"2023-10-10T13:04:06.016359Z","shell.execute_reply.started":"2023-10-10T13:04:06.010903Z"},"id":"9uc5Ex5mX3NQ","outputId":"1b64ad7b-3de2-4221-f52e-f6cfcd794cd0","trusted":true},"outputs":[],"source":["# NO_PREDICTS image results\n","# 9 timesteps\n","# 32 chars + 1 blank\n","prediction.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T13:04:09.857279Z","iopub.status.busy":"2023-10-10T13:04:09.856881Z","iopub.status.idle":"2023-10-10T13:04:09.992861Z","shell.execute_reply":"2023-10-10T13:04:09.991864Z","shell.execute_reply.started":"2023-10-10T13:04:09.857250Z"},"id":"ObYLSxKVX4hg","outputId":"e9fe221d-007b-486c-cda7-295445c8801d","trusted":true},"outputs":[],"source":["# use CTC decoder\n","out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n","                         greedy=True)[0][0])\n","# see the results\n","all_predictions =[]\n","i = 0\n","for x in out:\n","    print(\"original_text  = \", valid_orig_txt[i+OFFSET])\n","    print(\"predicted text = \", end = '')\n","    pred = \"\"\n","    for p in x:\n","        if int(p) != -1:\n","            pred += char_list[int(p)]\n","    print(pred)\n","    all_predictions.append(pred)\n","    i+=1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T13:04:44.508091Z","iopub.status.busy":"2023-10-10T13:04:44.507701Z","iopub.status.idle":"2023-10-10T13:05:05.642756Z","shell.execute_reply":"2023-10-10T13:05:05.641354Z","shell.execute_reply.started":"2023-10-10T13:04:44.508063Z"},"id":"cgU_Y9ifX7Q1","outputId":"fa41b3e5-5893-4de2-dec7-230e3e38b7b8","trusted":true},"outputs":[],"source":["#YOUR PART: WRITE TO PRINT OUT THE IMAGES IN NICE 2D ARRAY PLOT WITH ORIGINAL TEXT AND PREDICTED TEXT\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","plt.figure(figsize=(10,200))\n","col=0\n","row=1\n","gs1 = gridspec.GridSpec(NO_PREDICTS, 1)\n","# gs1.update(wspace=0.025, hspace=0.025)\n","for n in range(NO_PREDICTS):\n","    plt.subplot(gs1[n])\n","    plt.imshow(valid_img[n][:,:,0], cmap=\"gray_r\")\n","    plt.title(f\"Label {n}: \"+valid_orig_txt[n+OFFSET], fontsize=20, color=\"green\")\n","    plt.xlabel(f\"Prediction {n}: \"+all_predictions[n+OFFSET], fontsize=20, color=\"red\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T13:05:05.679073Z","iopub.status.busy":"2023-10-10T13:05:05.678417Z","iopub.status.idle":"2023-10-10T13:05:16.173935Z","shell.execute_reply":"2023-10-10T13:05:16.172655Z","shell.execute_reply.started":"2023-10-10T13:05:05.679037Z"},"trusted":true},"outputs":[],"source":["# !pip install editdistance\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T13:05:16.176989Z","iopub.status.busy":"2023-10-10T13:05:16.176001Z","iopub.status.idle":"2023-10-10T13:05:16.240997Z","shell.execute_reply":"2023-10-10T13:05:16.240088Z","shell.execute_reply.started":"2023-10-10T13:05:16.176931Z"},"id":"jEb5thqtX9kV","trusted":true},"outputs":[],"source":["import string\n","import unicodedata\n","import editdistance\n","\n","\n","def ocr_metrics(predicts, ground_truth, norm_accentuation=False, norm_punctuation=False):\n","    \"\"\"Calculate Character Error Rate (CER), Word Error Rate (WER) and Sequence Error Rate (SER)\"\"\"\n","\n","    if len(predicts) == 0 or len(ground_truth) == 0:\n","        return (1, 1, 1)\n","\n","    cer, wer, ser = [], [], []\n","\n","    for (pd, gt) in zip(predicts, ground_truth):\n","\n","        if norm_accentuation:\n","            pd = unicodedata.normalize(\"NFKD\", pd).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n","            gt = unicodedata.normalize(\"NFKD\", gt).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n","\n","        if norm_punctuation:\n","            pd = pd.translate(str.maketrans(\"\", \"\", string.punctuation))\n","            gt = gt.translate(str.maketrans(\"\", \"\", string.punctuation))\n","\n","        pd_cer, gt_cer = list(pd.lower()), list(gt.lower())\n","        dist = editdistance.eval(pd_cer, gt_cer)\n","        cer.append(dist / (max(len(pd_cer), len(gt_cer))))\n","\n","        pd_wer, gt_wer = pd.lower().split(), gt.lower().split()\n","        dist = editdistance.eval(pd_wer, gt_wer)\n","        wer.append(dist / (max(len(pd_wer), len(gt_wer))))\n","\n","        pd_ser, gt_ser = [pd], [gt]\n","        dist = editdistance.eval(pd_ser, gt_ser)\n","        ser.append(dist / (max(len(pd_ser), len(gt_ser))))\n","\n","    cer_f = sum(cer) / len(cer)\n","    wer_f = sum(wer) / len(wer)\n","    ser_f = sum(ser) / len(ser)\n","\n","    return (cer_f, wer_f, ser_f)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T13:05:50.356934Z","iopub.status.busy":"2023-10-10T13:05:50.355880Z","iopub.status.idle":"2023-10-10T13:05:50.370385Z","shell.execute_reply":"2023-10-10T13:05:50.369270Z","shell.execute_reply.started":"2023-10-10T13:05:50.356889Z"},"id":"VG_v7uolX-9V","trusted":true},"outputs":[],"source":["evaluate = ocr_metrics(predicts=all_predictions,\n","                                  ground_truth=valid_orig_txt,\n","                                  norm_accentuation=False,\n","                                  norm_punctuation=False)\n","\n","e_corpus = \"\\n\".join([\n","    \"Metrics:\",\n","    \"Character Error Rate: {}\".format(evaluate[0]),\n","    \"Word Error Rate:      {}\".format(evaluate[1]),\n","    \"Sequence Error Rate:  {}\".format(evaluate[2]),\n","])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T13:05:55.163873Z","iopub.status.busy":"2023-10-10T13:05:55.163500Z","iopub.status.idle":"2023-10-10T13:05:55.170538Z","shell.execute_reply":"2023-10-10T13:05:55.169303Z","shell.execute_reply.started":"2023-10-10T13:05:55.163843Z"},"id":"nurAoKm8YARl","outputId":"f736f600-9c98-44d0-93b2-905d49f769d8","trusted":true},"outputs":[],"source":["print(e_corpus)\n"]},{"cell_type":"markdown","metadata":{"id":"gZkVv4zIwj4h"},"source":["We got a pretty good results with CER at 4% and WER at 15%! I'm certain if I continue to apply for other techniques, this will help to reduce these numbers down. For example, I can try add to an attention layer between my CNN and RNN layers. "]},{"cell_type":"markdown","metadata":{"id":"hcPWqMCku4IL"},"source":["## Tensorboard"]},{"cell_type":"markdown","metadata":{"id":"njapnva1xJ-O"},"source":["As we can see, the loss for validation continue to increase for the first 6 epoches and sharply drop onwards and mostly stable all the way till epoch 80."]},{"cell_type":"markdown","metadata":{"id":"hULrv4GDsqzL"},"source":["Things to improve for the competition:\n","- Resize image logics with multiple image sizes (maybe as following):\n","    + find min, max of height and width\n","    + resize to a fixed height you want\n","    + calculate the max width of all resized images\n","    + padding to all images to that max width\n","- Combine the logic of preprocessing of train set and test set together\n","- Convert them to tfdataset pipeline (note that it is challenging since OpenCV won't work with tensor)\n","- Rework/redesign new CRNN model to fit with the new processed images. "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":4}
